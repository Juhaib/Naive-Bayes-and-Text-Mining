{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd752a1-99d5-4519-8a34-f141f99d9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if necessary\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba70cf0e-d43d-43b9-88b2-1f79396cee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"blogs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113922bb-9f88-42c7-92f6-c364b6e1d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract main content, excluding metadata\n",
    "def extract_content(text):\n",
    "    # Split text at first empty line (assumes body follows metadata)\n",
    "    content = re.split(r'\\n\\s*\\n', text, maxsplit=1)\n",
    "    return content[1] if len(content) > 1 else content[0]\n",
    "\n",
    "# Apply the extraction function\n",
    "df['Cleaned_Data'] = df['Data'].apply(extract_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f133835-b760-45ff-ae45-2214c144cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text cleaning\n",
    "df['Cleaned_Data'] = df['Cleaned_Data'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c7282f-e42d-4245-bf39-cfd367a48f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df['Cleaned_Data']).toarray()\n",
    "y = df['Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086df47a-4174-4db9-a7d0-6e62315f68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9c2e17-3fa5-4d14-a635-9f69ff775b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Model for Text Classification\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b035981-6ec7-4e8d-9a09-0c90f8499ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655\n",
      "Precision: 0.6824175258136331\n",
      "Recall: 0.655\n",
      "F1 Score: 0.652850417573629\n",
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.34      0.61      0.44        18\n",
      "           comp.graphics       0.58      0.61      0.59        18\n",
      " comp.os.ms-windows.misc       0.68      0.77      0.72        22\n",
      "comp.sys.ibm.pc.hardware       0.56      0.56      0.56        25\n",
      "   comp.sys.mac.hardware       0.55      0.52      0.54        21\n",
      "          comp.windows.x       0.82      0.56      0.67        25\n",
      "            misc.forsale       0.89      0.44      0.59        18\n",
      "               rec.autos       0.82      0.78      0.80        18\n",
      "         rec.motorcycles       0.74      0.88      0.80        16\n",
      "      rec.sport.baseball       0.67      0.78      0.72        18\n",
      "        rec.sport.hockey       0.71      1.00      0.83        15\n",
      "               sci.crypt       0.73      0.84      0.78        19\n",
      "         sci.electronics       0.33      0.56      0.42        16\n",
      "                 sci.med       0.79      0.88      0.83        17\n",
      "               sci.space       0.93      0.62      0.74        21\n",
      "  soc.religion.christian       0.77      0.74      0.76        23\n",
      "      talk.politics.guns       0.81      0.61      0.69        28\n",
      "   talk.politics.mideast       0.88      0.75      0.81        20\n",
      "      talk.politics.misc       0.57      0.72      0.63        18\n",
      "      talk.religion.misc       0.40      0.17      0.24        24\n",
      "\n",
      "                accuracy                           0.66       400\n",
      "               macro avg       0.68      0.67      0.66       400\n",
      "            weighted avg       0.68      0.66      0.65       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfdf4dd4-827f-4075-891f-b5247ac4fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment'] = df['Cleaned_Data'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1249ba67-977e-43ab-b42d-cde5958a9870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Distribution Across Categories:\n",
      " Sentiment                 Negative  Neutral  Positive\n",
      "Labels                                               \n",
      "alt.atheism                   0.30     0.03      0.67\n",
      "comp.graphics                 0.11     0.08      0.81\n",
      "comp.os.ms-windows.misc       0.12     0.08      0.80\n",
      "comp.sys.ibm.pc.hardware      0.12     0.02      0.86\n",
      "comp.sys.mac.hardware         0.15     0.05      0.80\n",
      "comp.windows.x                0.12     0.05      0.83\n",
      "misc.forsale                  0.12     0.07      0.81\n",
      "rec.autos                     0.17     0.06      0.77\n",
      "rec.motorcycles               0.21     0.04      0.75\n",
      "rec.sport.baseball            0.32     0.05      0.63\n",
      "rec.sport.hockey              0.35     0.04      0.61\n",
      "sci.crypt                     0.21     0.01      0.78\n",
      "sci.electronics               0.13     0.05      0.82\n",
      "sci.med                       0.30     0.02      0.68\n",
      "sci.space                     0.14     0.05      0.81\n",
      "soc.religion.christian        0.20      NaN      0.80\n",
      "talk.politics.guns            0.35     0.04      0.61\n",
      "talk.politics.mideast         0.22     0.03      0.75\n",
      "talk.politics.misc            0.23     0.03      0.74\n",
      "talk.religion.misc            0.18     0.02      0.80\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Distribution Analysis\n",
    "sentiment_counts = df.groupby('Labels')['Sentiment'].value_counts(normalize=True).unstack()\n",
    "print(\"\\nSentiment Distribution Across Categories:\\n\", sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b304eda3-58f8-43f3-8630-9655786c0151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to blogs_categories_with_sentiments.csv\n"
     ]
    }
   ],
   "source": [
    "# Save Results and Cleaned Dataset\n",
    "df.to_csv(\"blogs_categories_with_sentiments.csv\", index=False)\n",
    "print(\"\\nResults saved to blogs_categories_with_sentiments.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
